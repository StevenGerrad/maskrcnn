{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\research\\maskrcnn\\logs D:\\research\\maskrcnn\\mask_rcnn_coco.h5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\research\\maskrcnn\\logs D:\\research\\maskrcnn\\mask_rcnn_coco.h5\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1154: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1188: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "print(MODEL_DIR, COCO_MODEL_PATH)\n",
    "    \n",
    "import keras\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "#config.display()\n",
    "\n",
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax\n",
    "\n",
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes\n",
    "    \n",
    "    \n",
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()\n",
    "\n",
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.0001\n",
      "\n",
      "Checkpoint Path: D:\\research\\maskrcnn\\logs\\shapes20210111T2033\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)\n",
    "    \n",
    "# # Train the head branches\n",
    "# # Passing layers=\"heads\" freezes all layers except the head\n",
    "# # layers. You can also pass a regular expression to select\n",
    "# # which layers to train by name pattern.\n",
    "# model.train(dataset_train, dataset_val, \n",
    "#             learning_rate=config.LEARNING_RATE, \n",
    "#             epochs=1, \n",
    "#             layers='heads')\n",
    "\n",
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)\n",
    "\n",
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))\n",
    "\n",
    "\n",
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())\n",
    "\n",
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Load and display random samples\n",
    "# image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "# for image_id in image_ids:\n",
    "#     image = dataset_train.load_image(image_id)\n",
    "#     mask, class_ids = dataset_train.load_mask(image_id)\n",
    "#     visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not build a TypeSpec for <KerasTensor: shape=(None, None, 4) dtype=float32 (created by layer 'tf.math.truediv')> with type KerasTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7928c4edfc77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create model in training mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m model = modellib.MaskRCNN(mode=\"training\", config=config,\n\u001b[1;32m----> 3\u001b[1;33m                           model_dir=MODEL_DIR)\n\u001b[0m",
      "\u001b[1;32mD:\\research\\maskrcnn\\mrcnn\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mode, config, model_dir)\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_log_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\research\\maskrcnn\\mrcnn\\model.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, mode, config)\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;31m# Normalize coordinates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1886\u001b[0m             gt_boxes = KL.Lambda(lambda x: norm_boxes_graph(\n\u001b[1;32m-> 1887\u001b[1;33m                 x, K.shape(input_image)[1:3]))(input_gt_boxes)\n\u001b[0m\u001b[0;32m   1888\u001b[0m             \u001b[1;31m# 3. GT Masks (zero padded)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1889\u001b[0m             \u001b[1;31m# [batch, height, width, MAX_GT_INSTANCES]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[1;32m--> 952\u001b[1;33m                                                 input_list)\n\u001b[0m\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m         outputs = self._keras_tensor_symbolic_call(\n\u001b[1;32m-> 1091\u001b[1;33m             inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    867\u001b[0m                               build_graph=False)\n\u001b[0;32m    868\u001b[0m       outputs = nest.map_structure(\n\u001b[1;32m--> 869\u001b[1;33m           keras_tensor.keras_tensor_from_tensor, outputs)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_set_inputs'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\keras_tensor.py\u001b[0m in \u001b[0;36mkeras_tensor_from_tensor\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m    604\u001b[0m       \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m   \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras_tensor_cls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_keras_mask'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\keras_tensor.py\u001b[0m in \u001b[0;36mfrom_tensor\u001b[1;34m(cls, tensor)\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Fallback to the generic arbitrary-typespec KerasTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m       \u001b[0mtype_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_spec_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\type_spec.py\u001b[0m in \u001b[0;36mtype_spec_from_value\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m   raise TypeError(\"Could not build a TypeSpec for %r with type %s\" %\n\u001b[1;32m--> 554\u001b[1;33m                   (value, type(value).__name__))\n\u001b[0m\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not build a TypeSpec for <KerasTensor: shape=(None, None, 4) dtype=float32 (created by layer 'tf.math.truediv')> with type KerasTensor"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
